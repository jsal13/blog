<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Elasticsearch Tutorial: Part 2, Vector Search</title>

    <!-- CUSTOM CSS -->
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/pygmentize.css">

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

</head>

<body>

    

    <div id="post-content">
        <h1 id="the-elasticsearch-tutorial-part-2-vector-search">The Elasticsearch Tutorial: Part 2, Vector Search</h1>
<!-- ID: 202402240020 -->
<p>Last Updated: <em>2024-02-25</em></p>
<hr />
<p>These will be a continuation of notes from doing the <a href="https://www.elastic.co/search-labs/tutorials/search-tutorial/welcome">elasticsearch tutorial</a>.  This will be written as I go so that you can go on this <code>journey</code> with me.</p>
<p>What am I going to learn this time?  According to the tutorial:</p>
<blockquote>
<p>This section will introduce you to a different way of searching that leverages Machine Learning (ML) techniques to interpret meaning and context.</p>
</blockquote>
<p>Last time I learned some basic querying techniques which ultimately used regex-style matching and filtering; it seems that this time I'll be doing some sweet, sweet machine learning!</p>
<figure>
    <img src="../assets/images/elasticsearch-tutorial-02_decision_tree.jpg"
         alt="A decision tree.">
    <figcaption>A decision tree.</figcaption>
</figure>

<h2 id="embeddings">Embeddings</h2>
<p>The tutorial defines the term <strong>embedding</strong> as</p>
<blockquote>
<p>a vector (an array of numbers) that represents real-world objects such as words, sentences, images or videos.</p>
</blockquote>
<p>There'll no doubt be some examples soon if you haven't seen this kind of thing before.  One cool thing you can do with embeddings is define and then calculate <strong>distances</strong> between them.  This is done in a similar way as you might do in school:</p>
<p>$$
\begin{array}{ll}
A &amp;= (0, 0, 1)\\
B &amp;= (0, 1, 0)\\
d(A, B) &amp;= \sqrt{(0 - 0)^2 + (0 - 1)^2 + (1 - 0)^2} = \sqrt{2}\\
\end{array}
$$</p>
<p>For \(d(A, B)\) I used a Euclidean distance, but any <a href="https://en.wikipedia.org/wiki/Distance#Mathematical_formalization">distance function</a> would work equally well.</p>
<p>Why do I care about finding the distances between embeddings?  This might tell them if the two terms (or pictures, or videos, or whatever) are <em>similar</em> to each other in some respect.</p>
<p>The tutorial goes on and tells me that I'll use embeddings to help find concepts that are similar to one-another instead of using the keyword searching I've been doing so far.</p>
<h2 id="generating-embeddings">Generating Embeddings</h2>
<p>As soon as I get onto the next page of the tutorial, it tells me to go install <a href="https://www.sbert.net/">SentenceTransformers</a>.  I've heard of this before but haven't worked with it.</p>
<p>The tutorial tells me to select a pre-trained model.  Sure, that's reasonable, I don't want to have to spend all my time training this thing &mdash; and I almost certainly don't have enough data to train it on!  The tutorial recommends the <code>all-MiniLM-L6-v2</code> model on <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">huggingface</a> and I have no strong opposition.</p>
<figure>
    <img src="../assets/images/elasticsearch-tutorial-02_model_tags.png"
         alt="Model Tags.">
    <figcaption>Wow, trivia qa <b>and</b> yahoo answers?</figcaption>
</figure>

<p>I create a new python file in my Elasticsearch toy folder and copy their code down to load the model:</p>
<div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>

<p>Running the code gives me a bunch of logs of things downloading which I assume must be the model, or something equally magical.  I'm now able to generate embeddings!</p>
<div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">embedding</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;The quick brown fox jumps over the lazy dog&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>

<p>I take a sip of water and check the output.</p>
<div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div><pre><span></span><code>[ 3.54967788e-02  6.12862594e-02  5.26920781e-02
  ...
  (A ton of rows!)
  ...
  3.31014022e-02 -3.06695923e-02    8.67674053e-02]
</code></pre></div></td></tr></table></div>

<p>Oh, wow.  Taking a look at the model page again, I notice</p>
<blockquote>
<p>This is a sentence-transformers model: It maps sentences &amp; paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.</p>
</blockquote>
<p>That tracks.  The model has 384 dimensions, so this embedding will be an array with 384 elements.</p>
<h2 id="field-types">Field Types</h2>
<p>This part of the tutorial goes into how ES tries its best to create field mappings in the index that make sense &mdash; like the "text" and "date" fields in the previous section &mdash; but that I can explicitly create fields with a certain type if I want.  They give the code for that here:</p>
<div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Search</span><span class="p">:</span>
    <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">create_index</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">es</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s1">&#39;my_documents&#39;</span><span class="p">,</span> <span class="n">ignore_unavailable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">es</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s1">&#39;my_documents&#39;</span><span class="p">,</span> <span class="n">mappings</span><span class="o">=</span><span class="p">{</span>
            <span class="s1">&#39;properties&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s1">&#39;embedding&#39;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;dense_vector&#39;</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">})</span>
</code></pre></div></td></tr></table></div>

<p>Notice that the embedding type is <em>dense vector</em>.  I thought to myself: what <em>is</em> a dense vector?  It turns out that this is the opposite of a <em>sparse vector</em>: whereas a sparse vector primarily consists of zero entries with only a few non-zero entries, a dense vector contains a large number of non-zero entries.  Given the vector above is mostly non-zero entries, it makes sense that this embedding will pop out dense vectors and, therefore, this embedding type makes sense for my index.</p>
<p>There's a few optional parameters that I can set if I want: <code>dims</code> (the dimension of the vector), <code>index</code> (vectors should be indexed for searching), and <code>similarity</code> (the distance function to use, where <code>cosine</code> and <code>dot_product</code> are the most common).</p>
<p>In almost exactly the same way I was able to upload my documents when I did the "Full-Text" tutorial, I'm able to upload my embeddings.  Code from the tutorial:</p>
<div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Search</span><span class="p">:</span>
    <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">get_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">insert_document</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">document</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">es</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s1">&#39;my_documents&#39;</span><span class="p">,</span> <span class="n">document</span><span class="o">=</span><span class="p">{</span>
            <span class="o">**</span><span class="n">document</span><span class="p">,</span>
            <span class="s1">&#39;embedding&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s1">&#39;summary&#39;</span><span class="p">]),</span>
        <span class="p">})</span>

    <span class="k">def</span> <span class="nf">insert_documents</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">):</span>
        <span class="n">operations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
            <span class="n">operations</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;_index&#39;</span><span class="p">:</span> <span class="s1">&#39;my_documents&#39;</span><span class="p">}})</span>
            <span class="n">operations</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="o">**</span><span class="n">document</span><span class="p">,</span>
                <span class="s1">&#39;embedding&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s1">&#39;summary&#39;</span><span class="p">]),</span>
            <span class="p">})</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">es</span><span class="o">.</span><span class="n">bulk</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">operations</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>

<p>I haven't mentioned this yet, but it does bother me that there are no type-hints in this code.  On one hand I understand it could be a bit distracting but on the other hand I like to know what types my parameters are.  It helps me understand the "flow" of the code.  Alas, life isn't fair sometimes.</p>
<h2 id="k-nearest-neighbor-search">k-Nearest Neighbor Search</h2>
<p>If you haven't seen kNN before, I've drawn this picture <em>just for you</em>.  </p>
<figure>
    <img src="../assets/images/elasticsearch-tutorial-02_knn.png"
         alt="kNN.">
    <figcaption>The black circle is the "new" thing to be classified.  If k = 3, look at the 3 nearest neighbors to the black dot: pink, green, green.  Since there are more greens, the black dot is classified as green.  If, on the other hand, k = 7, then looking at the 7 nearest neighbors (all of the other dots) give us 5 pink and 2 green: in this case, the black dot is classified as green.  It is important to pick the right k for the job, and that's not always easy!</figcaption>
</figure>

<p>Luckily, ES is going to do the work for me.  By default, ES will use <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> as the distance function and find all of the embeddings close to whatever the query is with respect to that distance function.  ES has kNN as part of the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html#search-api-knn">default search API</a>.</p>
<h2 id="hybrid-search">Hybrid Search</h2>
<p>This section talks about <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html">Reciprocal Rank Fusion</a>.  Seems like a fancy polling-type thing where a few different result sets can be compared.  The algorithm is suprisingly simple:</p>
<div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">score</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">queries</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">result</span><span class="p">(</span><span class="n">q</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span> <span class="n">k</span> <span class="o">+</span> <span class="n">rank</span><span class="p">(</span> <span class="n">result</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="n">d</span> <span class="p">)</span> <span class="p">)</span>
<span class="k">return</span> <span class="n">score</span>

<span class="c1"># where</span>
<span class="c1"># k is a ranking constant</span>
<span class="c1"># q is a query in the set of queries</span>
<span class="c1"># d is a document in the result set of q</span>
<span class="c1"># result(q) is the result set of q</span>
<span class="c1"># rank( result(q), d ) is d&#39;s rank within the result(q) starting from 1</span>
</code></pre></div></td></tr></table></div>

<p>I don't have a great image in my head of what this does or why it works yet but popping in a few numbers convinces me it's at least directionally reasonable.</p>
<p>The way to use this is remarkably easy.  Check out [<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html#rrf-api">this GET payload</a>] example:</p>
<div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;query&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;term&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;text&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;shoes&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;knn&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;field&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;vector&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;query_vector&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">1.25</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mf">3.5</span><span class="p">],</span>
<span class="w">        </span><span class="nt">&quot;k&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;num_candidates&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;rank&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;rrf&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;window_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;rank_constant&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div></td></tr></table></div>

<blockquote>
<p>In the above example, we first execute the kNN search to get its global top 50 results. Then we execute the query to get its global top 50 results. Afterwards, on a coordinating node, we combine the knn search results with the query results and rank them based on the RRF method to get the final top 10 results.</p>
</blockquote>
<p>Text search, kNN, and RRF all working together like one big happy family.  Cool stuff, ES.</p>
<h2 id="next-time">Next Time</h2>
<p>I didn't do much "hands on" work this time since the kNN + RRF were quite similar to the full-text search in terms of the API and it was a bit more boring than I thought to bulk upload a bunch of sample embeddings and see which were close to one-another.  I'm sure if I had a better dataset than "random phrases I thought of" it would be neat.  Maybe in the last part of this series I'll find a nice dataset and answer some questions in it.</p>
<p>The "Vector Search" part of the ES tutorial ends here.  In the next post I'll cover the next part of the tutorial: Semantic Search.  This is the part with that ELSER Model, so I'll finally get to learn what the acronym stands for!</p>
    </div>
</body>

</html>