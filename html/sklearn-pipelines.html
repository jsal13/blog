<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Using Sklean Pipelines for Fun and Profit</title>

    <!-- CUSTOM CSS -->
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/pygmentize.css">

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

</head>

<body>

    

    <div id="post-content">
        <h1 id="using-sklean-pipelines-for-fun-and-profit">Using Sklean Pipelines for Fun and Profit</h1>
<!-- ID: 202306060010 -->
<p>Last Updated: <em>2023-06-06</em></p>
<hr />
<h2 id="introduction">Introduction</h2>
<p>Sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">pipelines</a> are an elegant way to organize modeling workflows.  It also provides an "at-a-glance" picture of what is going into the current model &mdash; something your future self will thank you for when you read that notebook back in six months and have no idea what anything is doing.</p>
<h2 id="getting-toy-data-to-play-with">Getting Toy Data To Play With</h2>
<p>I'll be working with the cute <a href="https://github.com/allisonhorst/palmerpenguins">Penguins</a> dataset which <strong>seaborn</strong> can load.  The "goal" of my classifier: try to predict the sex of the penguin, given the rest of the features.  <strong>This model will purposely be simplistic: it will serve only to help explain how to set up pipelines.</strong></p>
<h2 id="loading-the-data">Loading the Data</h2>
<p>I made a CSV of the data and a script to load it.  <strong>In this post, I've opted to use <a href="https://pola.rs/">Polars</a> over <a href="https://pandas.pydata.org/docs/index.html">Pandas</a>.</strong>  </p>
<div class="admonition note">
<p class="admonition-title">Pandas?</p>
<p>If you prefer <a href="https://pandas.pydata.org/docs/index.html">Pandas</a> there should only be changes required for this next code block, but all the code after this code block should work without changes.</p>
</div>
<pre class="codehilite"><code class="language-python linenums">import polars as pl
from sklearn.model_selection import train_test_split

df = pl.read_csv(&quot;./datasets/penguins.csv&quot;, null_values=[&quot;NA&quot;], dtypes={&quot;year&quot;: str})
df = df.drop_nulls(subset=[&quot;sex&quot;])

x = df.drop(&quot;sex&quot;)
y = df.select((pl.col(&quot;sex&quot;) == &quot;male&quot;).alias(&quot;target&quot;).cast(int))

# A weird quirk for sklearn.
y = y.to_numpy().ravel()

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.33, random_state=1234
)
</code></pre>

<p>The file contains null values of the form "NA" so I set <code>null_values=["NA"]</code> to make Polars understand that those are nulls.  Additionally, Polars will think that a year is an <code>int</code> but I want it as a categorical so we set the dtype to be <code>str</code>.  Since I'm predicting on <strong>sex</strong>, I'm going to drop any rows that have a null in that column since they won't give me any new predictive information.</p>
<p>I define <code>x</code> and <code>y</code> in the next few lines.  The <code>df.select(...)</code> part is a bit strange if you haven't seen it before, so let me go through it:</p>
<ol>
<li><code>pl.col("sex")</code> picks out the <strong>sex</strong> column,</li>
<li><code>pl.col("sex") == "male"</code> compares the column to the string <code>"male"</code>, resulting in a column which is either <code>True</code> or <code>False</code>,</li>
<li><code>.alias("target")</code> gives the name <strong>target</strong> to this new boolean column,</li>
<li><code>.cast(int)</code> casts the new <strong>target</strong> column as an <code>int</code> column (<code>True</code> goes to <code>1</code>, <code>False</code> goes to <code>0</code>),</li>
<li><code>df.select(...)</code> wraps around the whole thing to say, "I'm working with <code>df</code>, and we're going to do something with the columns of this dataframe.`  See <a href="https://docs.pola.rs/py-polars/html/reference/dataframe/api/polars.DataFrame.select.html">the docs</a> for a bit more detail on this and some examples.</li>
</ol>
<p>This might seem a bit more complicated compared to the Pandas version, which goes something like <code>(df["sex"] == "male").astype(int)</code> but as the number or complexity of derived columns increases I find that Polars is much more readable.</p>
<p>Either way, looking at the next part, what the heck is this?</p>
<pre class="codehilite"><code class="language-python linenums">y = y.to_numpy().ravel()
</code></pre>

<p>I do this because during the fitting process later, sklearn will require my target values to be a 1D array.  Currently, it is a Polars Series, which, when I make it into a <em>numpy</em> array will become a <code>(num_rows, 1)</code> ndarray.  Sklearn requires this be <code>(num_rows,)</code> &mdash; that is, a flat array &mdash; which I can get using <code>.ravel()</code>.  It's weird but it makes sense if you look at the underlying arrays (pre- and post-ravel). </p>
<p>Last, I do a train-test split.</p>
<pre class="codehilite"><code class="language-python linenums">x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.33, random_state=1234
)
</code></pre>

<p>Nothing wild here.  This gives me two different sets of feature and target sets: one to use for training, one to use for testing.</p>
<h2 id="pipelines">Pipelines</h2>
<p>A <code>Pipeline</code> takes a list of 2-tuples <code>(name, transform)</code> where a <code>transform</code> in Sklean is anything which has implemented the <code>fit</code>/<code>transform</code> methods (see <a href="https://scikit-learn.org/stable/data_transforms.html">the docs</a> for more on this).  The pattern I'll use is roughly like this:</p>
<div class="admonition warning">
<p class="admonition-title">This is Pseudo-code.</p>
</div>
<pre class="codehilite"><code class="language-python linenums"># List of columns to transform in this pipeline.
some_column_list = [...]

# List of transformations, in order, to apply.
pipeline = Pipeline(
    [
        (&quot;name_of_transformer_1&quot;, Transformer1()),
        (&quot;name_of_transformer_2&quot;, Transformer2()),

    ]
)

# Make the transformer, which will apply ALL transformations.
# In this case, there is only the one above.
col_transformer = ColumnTransformer(
    [
        (&quot;col_pipeline&quot;, pipeline, some_column_list),
    ]
)

# Fit the transformer with training data.
col_transformer.fit(x_train, y_train)
</code></pre>

<p>This is the gist of how pipelines work: pick columns to apply the transforms to, make a pipeline to list the transformers to apply, and then apply all the pipelines using the (I think) awkwardly named <code>ColumnTransformer</code>.  It's going to look like a lot, but all I'm doing is filling out the above with a few standard transformers.  Right now, you don't need to know what these transformers even <em>do</em>, so long as you know they <em>do something</em>.</p>
<pre class="codehilite"><code class="language-python linenums">from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Make pipelines for specific columns.
cols_numeric = [&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, &quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;]
pipeline_numeric = Pipeline(
    [
        (&quot;impute_w_mean&quot;, SimpleImputer(strategy=&quot;mean&quot;)),
        (&quot;scale_normal&quot;, StandardScaler()),
    ]
)

cols_categorical = [&quot;species&quot;, &quot;island&quot;, &quot;year&quot;]
pipeline_categorical = Pipeline(
    [
        (&quot;impute_w_most_frequent&quot;, SimpleImputer(strategy=&quot;most_frequent&quot;)),
        (&quot;one_hot_encode&quot;, OneHotEncoder(handle_unknown=&quot;ignore&quot;, sparse_output=False)),
    ]
)

# Put it all together into a transformer.
preprocessing_transformer = ColumnTransformer(
    [
        (&quot;numeric&quot;, pipeline_numeric, cols_numeric),
        (&quot;categorical&quot;, pipeline_categorical, cols_categorical),
    ]
)

# Fit the transformer with training data.
preprocessing_transformer.fit(x_train, y_train)

# ---
print(preprocessing_transformer.get_feature_names_out(), end=&quot;\n\n&quot;)
print(preprocessing_transformer.fit_transform(x_train, y_train))
</code></pre>

<p>At the end, I printed the values:</p>
<pre class="codehilite"><code class="language-text linenums">['numeric__bill_length_mm' 'numeric__bill_depth_mm'
 'numeric__flipper_length_mm' 'numeric__body_mass_g'
 'categorical__species_Adelie' 'categorical__species_Chinstrap'
 'categorical__species_Gentoo' 'categorical__island_Biscoe'
 'categorical__island_Dream' 'categorical__island_Torgersen'
 'categorical__year_2007' 'categorical__year_2008'
 'categorical__year_2009']

[[-1.00173392  0.34890155 -1.47813963 ...  1.          0.
   0.        ]
 [ 0.00368887  0.08989674  1.22603896 ...  0.          1.
   0.        ]
 [-0.4716019   1.17771697 -0.26837552 ...  0.          1.
   0.        ]
 ...
 [ 1.53924369  1.48852275  0.22976264 ...  0.          1.
   0.        ]
 [ 0.93599001 -0.47991386  1.9376649  ...  0.          1.
   0.        ]
 [ 0.46069923 -0.27271    -0.69535109 ...  0.          1.
   0.        ]]
</code></pre>

<p>Great.  The <code>.fit_transform(...)</code> has given me the latter array which is ready to be popped into a model.  I'll do just that with a <code>RandomForestClassifier</code>, though any classifier would work for my purposes in this post.</p>
<pre class="codehilite"><code class="language-python linenums">from sklearn.ensemble import RandomForestClassifier

# Make a RF Classifier and pipeline it with the preprocessor.
rf_clf = RandomForestClassifier()

preprocess_model_pipeline = Pipeline(
    [(&quot;preprocessing&quot;, preprocessing_transformer), (&quot;classifier&quot;, rf_clf)]
)

preprocess_model_pipeline.fit(x_train, y_train)
y_predicted = preprocess_model_pipeline.predict(x_test)
</code></pre>

<p>I've created another pipeline which includes the preprocessing (the thing I defined above) and our new classifier.  The raw data is processed first by the <code>preprocessing_transformer</code>, then spit out into the <code>rf_clf</code> RF Classifier.  One cool thing is that I don't need to keep <code>.fit</code>-ing everything, I can call <code>.fit(...)</code> on the pipeline and it'll push the data through!  Similarly, I can call <code>.predict(...)</code> on the pipeline and it will do what I expect: give me an array of predictions for my test values.</p>
<h2 id="scoring-the-model">Scoring the Model</h2>
<p>How did the model do?</p>
<pre class="codehilite"><code class="language-python linenums">from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

scores = [
    (&quot;accuracy&quot;, accuracy_score(y_test, y_predicted)),
    (&quot;precision&quot;, precision_score(y_test, y_predicted)),
    (&quot;recall&quot;, recall_score(y_test, y_predicted)),
    (&quot;f1&quot;, f1_score(y_test, y_predicted)),
]

for score in scores:
    print(score[0], &quot;:&quot;, round(score[1],4))
</code></pre>

<pre class="codehilite"><code class="language-text linenums">accuracy : 0.9
precision : 0.8627
recall : 0.9167
f1 : 0.8889
</code></pre>

<p>Not <em>terrible</em> for some standard transformers and random forest classification!</p>
<h2 id="adding-to-pipelines">Adding to Pipelines</h2>
<p>Besides keeping everything together, pipelines allow the modeler to see what's happening to the data at what stage and easily add new transformers to the flow.  For example, what if I wanted to use some other strategy with our <code>SimpleImputer</code>?  What if I wanted to add a second <code>Scaler</code> for some reason?  What if I wanted to add a <a href="https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers">custom transformer</a>?</p>
<p>This becomes a matter of adding something onto a list.  If you've seen notebooks where transforms are peppered across multiple random cells in a Jupyter notebook (or, worse, in multiple Jupyter Notebooks!), then it's easy to appreciate how nice and clean this makes things.</p>
<p><em>Tidy is good.  Be tidy.</em></p>
    </div>
</body>

</html>